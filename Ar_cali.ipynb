{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1caabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, time\n",
    "\n",
    "cv2.namedWindow(\"Image Feed\")\n",
    "cv2.moveWindow(\"Image Feed\", 159, -25)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# setup Cam\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_FPS,40)\n",
    "\n",
    "prev_frame_time = time.time()\n",
    "\n",
    "cal_image_count = 0\n",
    "frame_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    #process\n",
    "    frame_count +=1\n",
    "    \n",
    "    if frame_count == 30:\n",
    "        cv2.imwrite(\"cal_image_\" + str(cal_image_count) + \".jpg\", frame)\n",
    "        cal_image_count +=1\n",
    "        frame_count = 0\n",
    "        \n",
    "    #calculate fps and display\n",
    "    new_frame_time = time.time()\n",
    "    fps = 1/(new_frame_time - prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "    cv2.putText(frame, \"FPS \" + str(int(fps)), (10, 40), cv2.FONT_HERSHEY_PLAIN, 3, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Image Feed\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"): break\n",
    "cap.release()\n",
    "cap.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0dcba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "\n",
    "cb_width = 7\n",
    "\n",
    "cb_height = 7\n",
    "\n",
    "cb_square_size = 23.7\n",
    "\n",
    "#termination criteria\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0), (6,5,0)\n",
    "cb_3D_points = np.zeros((cb_width*cb_height, 3), np.float32)\n",
    "cb_3D_points[:,:2] = np.mgrid[0:cb_width, 0:cb_height].T.reshape(-1,2)*cb_square_size\n",
    "\n",
    "#Arrays to store object points and image points from all the images.\n",
    "list_cb_3d_points = [] # 3d point in real world space\n",
    "list_cb_2d_img_points = [] # 2d points in image plane.\n",
    "\n",
    "list_images = glob.glob('*.jpg')\n",
    "\n",
    "for frame_name in list_images: \n",
    "    img = cv2.imread(frame_name)\n",
    "\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chess board corners\n",
    "\n",
    "    ret, corners = cv2.findChessboardCorners (gray, (7,7), None)\n",
    "\n",
    "    #If found , add object points, image points (after refining them)\n",
    "\n",
    "    if ret == True:\n",
    "\n",
    "        list_cb_3d_points.append(cb_3D_points)\n",
    "\n",
    "        corners2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)\n",
    "        list_cb_2d_img_points.append(corners2)\n",
    "\n",
    "        #Draw and display the corners\n",
    "\n",
    "        cv2.drawChessboardCorners(img, (cb_width, cb_height), corners2, ret)\n",
    "        cv2.imshow('img',img)\n",
    "        cv2.waitKey(500)\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(list_cb_3d_points, list_cb_2d_img_points, gray.shape[::-1],None, None)\n",
    "\n",
    "print(\"Calibration Matrix: \")\n",
    "print(mtx)\n",
    "print(\"Disortion: \", dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff51e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('camera_cal.npy', 'wb') as f:\n",
    "    np.save(f, mtx)\n",
    "    np.save(f, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db847e41",
   "metadata": {},
   "source": [
    "Calibration Matrix: \n",
    "[[583.52371648   0.         296.37015552]\n",
    " [  0.         627.94558759 358.30354553]\n",
    " [  0.           0.           1.        ]]\n",
    "Disortion:  [[ 0.37230818 -0.4038794   0.13492957 -0.00807964  0.30939183]]\n",
    "â€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1abb2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ccdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-contrib-python opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33665433",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python==4.6.0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4fd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "\n",
    "marker_size = 100\n",
    "# the camera matrix and camera distortion shouldm be numpy array type not list type\n",
    "camera_matrix = np.array([[583.52371648,   0.,         296.37015552], \n",
    " [  0.,         627.94558759, 358.30354553],\n",
    " [  0.,           0.,           1.        ]])\n",
    "camera_distortion = np.array([[ 0.37230818, -0.4038794,   0.13492957, -0.00807964,  0.30939183]])\n",
    "\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "parameters =  aruco.DetectorParameters()\n",
    "detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "camera_width = 640\n",
    "camera_height = 480\n",
    "camera_frame_rate = 40\n",
    "\n",
    "cap.set(2, camera_width)\n",
    "cap.set(4, camera_height)\n",
    "cap.set(5, camera_frame_rate)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    corners, ids, rejected = detector.detectMarkers(gray_frame)\n",
    "\n",
    "    if ids is not None: # detecting if any problemetic frames come into code we can use a try except statement to get rid of problematic frames\n",
    "        aruco.drawDetectedMarkers(frame, corners)\n",
    "        #ret = aruco.estimatePoseSingleMarkers(corners,marker_size,cameraMatrix=cameraMatrix,distCoeffs=cameraDistortion)\n",
    "        ret = aruco.estimatePoseSingleMarkers(corners, marker_size,cameraMatrix= camera_matrix,distCoeffs= camera_distortion)\n",
    "        rvec,tvec = ret[0][0,0,:], ret[1][0,0,:]\n",
    "        aruco.drawAxis(frame, camera_matrix, camera_distortion, rvec, tvec, 100)\n",
    "        tvec_str = \"x=%4.0f   y=%4.0f  z=%4.0f\"%(tvec[0], tvec[1], tvec[2])\n",
    "        cv2.putText(frame, tvec_str, (20, 460), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        \n",
    "    cv2.imshow('frame',frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744902b-1356-4e84-bfe1-08d0d18f365f",
   "metadata": {},
   "source": [
    "b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cff0b2e-be10-43a2-823b-53d5310342e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def focalMM_to_focalPixel(focalMM, pixelPitch):\n",
    "    return focalMM / pixelPitch\n",
    "\n",
    "# Camera matrix and distortion coefficients\n",
    "camera_matrix = np.array([\n",
    "    [583.52371648, 0., 296.37015552], \n",
    "    [0., 627.94558759, 358.30354553],\n",
    "    [0., 0., 1.]\n",
    "])\n",
    "camera_distortion = np.array([0.37230818, -0.4038794, 0.13492957, -0.00807964, 0.30939183])\n",
    "\n",
    "# Define the dictionary and detector parameters\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "parameters = aruco.DetectorParameters()\n",
    "\n",
    "# Define the 3D coordinates of the marker corners in meters\n",
    "marker_size = 0.1  # size of the marker in meters\n",
    "obj_points = np.array([\n",
    "    [-marker_size / 2, marker_size / 2, 0],\n",
    "    [ marker_size / 2, marker_size / 2, 0],\n",
    "    [ marker_size / 2, -marker_size / 2, 0],\n",
    "    [-marker_size / 2, -marker_size / 2, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "camera_width = 640\n",
    "camera_height = 480\n",
    "camera_frame_rate = 40\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, camera_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_height)\n",
    "cap.set(cv2.CAP_PROP_FPS, camera_frame_rate)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ArUco markers\n",
    "    detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "    corners, ids, rejected = detector.detectMarkers(gray_frame)\n",
    "\n",
    "    if ids is not None:\n",
    "        # Draw detected markers\n",
    "        aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            # Estimate pose of each marker using SOLVEPNP_IPPE_SQUARE\n",
    "            success, rvec, tvec = cv2.solvePnP(obj_points, corners[i][0], camera_matrix, camera_distortion, flags=cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "\n",
    "            if success:\n",
    "                rvec = rvec.reshape((3, 1))  # Ensure rvec is 3x1\n",
    "                tvec = tvec.reshape((3, 1))  # Ensure tvec is 3x1\n",
    "\n",
    "                # Draw axis for each marker\n",
    "                cv2.drawFrameAxes(frame, camera_matrix, camera_distortion, rvec, tvec, 0.1)\n",
    "\n",
    "                # Convert translation from meters to millimeters\n",
    "                tvec_mm = tvec * 1000  # Convert to millimeters\n",
    "\n",
    "                # Calculate camera position\n",
    "                rmat = cv2.Rodrigues(rvec)[0]\n",
    "                camera_position = -np.matrix(rmat).T @ np.matrix(tvec)\n",
    "\n",
    "                # Extract scalar values from the translation vector in mm\n",
    "                tvec_str = \"x={:.2f} mm y={:.2f} mm z={:.2f} mm\".format(tvec_mm[0][0], tvec_mm[1][0], tvec_mm[2][0])\n",
    "                cv2.putText(frame, tvec_str, (20, 460), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Extract rotation angles\n",
    "                r = Rotation.from_rotvec([rvec[0][0], rvec[1][0], rvec[2][0]])\n",
    "                rot = r.as_euler('xyz', degrees=True)\n",
    "\n",
    "                tx = round(camera_position[0, 0] * 1000, 2)  # Convert to millimeters\n",
    "                ty = round(camera_position[1, 0] * 1000, 2)  # Convert to millimeters\n",
    "                tz = round(camera_position[2, 0] * 1000, 2)  # Convert to millimeters\n",
    "\n",
    "                rx = round(180 - rot[0], 2)\n",
    "                ry = round(rot[1], 2)\n",
    "                rz = round(rot[2], 2)\n",
    "\n",
    "                rot_str = \"rx={:.2f} ry={:.2f} rz={:.2f}\".format(rx, ry, rz)\n",
    "                cv2.putText(frame, rot_str, (20, 420), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0e7d93-fd92-417f-937e-bdfedc830c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n",
      "['ArucoDetector', 'Board', 'CORNER_REFINE_APRILTAG', 'CORNER_REFINE_CONTOUR', 'CORNER_REFINE_NONE', 'CORNER_REFINE_SUBPIX', 'CharucoBoard', 'CharucoDetector', 'CharucoParameters', 'DICT_4X4_100', 'DICT_4X4_1000', 'DICT_4X4_250', 'DICT_4X4_50', 'DICT_5X5_100', 'DICT_5X5_1000', 'DICT_5X5_250', 'DICT_5X5_50', 'DICT_6X6_100', 'DICT_6X6_1000', 'DICT_6X6_250', 'DICT_6X6_50', 'DICT_7X7_100', 'DICT_7X7_1000', 'DICT_7X7_250', 'DICT_7X7_50', 'DICT_APRILTAG_16H5', 'DICT_APRILTAG_16h5', 'DICT_APRILTAG_25H9', 'DICT_APRILTAG_25h9', 'DICT_APRILTAG_36H10', 'DICT_APRILTAG_36H11', 'DICT_APRILTAG_36h10', 'DICT_APRILTAG_36h11', 'DICT_ARUCO_MIP_36H12', 'DICT_ARUCO_MIP_36h12', 'DICT_ARUCO_ORIGINAL', 'DetectorParameters', 'Dictionary', 'Dictionary_getBitsFromByteList', 'Dictionary_getByteListFromBits', 'GridBoard', 'RefineParameters', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_native', 'drawDetectedCornersCharuco', 'drawDetectedDiamonds', 'drawDetectedMarkers', 'extendDictionary', 'generateImageMarker', 'getPredefinedDictionary']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)\n",
    "print(dir(cv2.aruco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f412d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo nano  is nano editior\n",
    "#nano is a GNU editior \n",
    "#chmod77 what does it do \n",
    "#C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_12456\\1244901739.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
    "#  frame_np = np.array(frame)\n",
    "#---------------------------------------------------------------------------\n",
    "#error                                     Traceback (most recent call last)\n",
    "#Cell In[6], line 59\n",
    "#     56 #    frame = cv2.resize(frame,(width,height))\n",
    "#     58     frame_np = np.array(frame)\n",
    "#---> 59     gray_img = cv2.cvtColor(frame_np,cv2.COLOR_BGR2GRAY)\n",
    "#     60     ids=''\n",
    "#     61     corners, ids, rejected = aruco.detectMarkers(image=gray_img,dictionary=aruco_dict,parameters=parameters)\n",
    "#\n",
    "#error: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n",
    "#> Overload resolution failed:\n",
    "#>  - src data type = object is not supported\n",
    "#>  - Expected Ptr<cv::UMat> for argument 'src'\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from cv2 import aruco\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "dictionary = aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_250)\n",
    "\n",
    "while(True):\n",
    "    #Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    parameters = aruco.DetectorParameters()\n",
    "    detector = aruco.ArucoDetector(dictionary, parameters)\n",
    "    corners, ids, rejectedImgPoints = detector.detectMarkers(gray)\n",
    "    frame_markers = aruco.drawDetectedMarkers (frame.copy(), corners, ids)\n",
    "\n",
    "    cv2.imshow('Aruco Markers', frame_markers)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6339c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57720d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d697c010-cf6e-4742-ba0f-28e7adc943d6",
   "metadata": {},
   "source": [
    "Detect and feed to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "713009f1-8d2b-4ec9-8164-0380adcd764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "from scipy.spatial.transform import Rotation\n",
    "import time\n",
    "\n",
    "def focalMM_to_focalPixel(focalMM, pixelPitch):\n",
    "    return focalMM / pixelPitch\n",
    "\n",
    "# Camera matrix and distortion coefficients\n",
    "camera_matrix = np.array([\n",
    "    [583.52371648, 0., 296.37015552], \n",
    "    [0., 627.94558759, 358.30354553],\n",
    "    [0., 0., 1.]\n",
    "])\n",
    "camera_distortion = np.array([0.37230818, -0.4038794, 0.13492957, -0.00807964, 0.30939183])\n",
    "\n",
    "# Define the dictionary and detector parameters\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "parameters = aruco.DetectorParameters()\n",
    "\n",
    "# Define the 3D coordinates of the marker corners in meters\n",
    "marker_size = 0.1  # size of the marker in meters\n",
    "obj_points = np.array([\n",
    "    [-marker_size / 2, marker_size / 2, 0],\n",
    "    [ marker_size / 2, marker_size / 2, 0],\n",
    "    [ marker_size / 2, -marker_size / 2, 0],\n",
    "    [-marker_size / 2, -marker_size / 2, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "camera_width = 640\n",
    "camera_height = 480\n",
    "camera_frame_rate = 40\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, camera_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_height)\n",
    "cap.set(cv2.CAP_PROP_FPS, camera_frame_rate)\n",
    "\n",
    "prev_time = time.time()\n",
    "frame_count = 0\n",
    "fps = 0  # Variable to store the FPS value\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ArUco markers\n",
    "    detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "    corners, ids, rejected = detector.detectMarkers(gray_frame)\n",
    "\n",
    "    if ids is not None:\n",
    "        # Draw detected markers\n",
    "        aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            # Estimate pose of each marker using SOLVEPNP_IPPE_SQUARE\n",
    "            success, rvec, tvec = cv2.solvePnP(obj_points, corners[i][0], camera_matrix, camera_distortion, flags=cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "\n",
    "            if success:\n",
    "                rvec = rvec.reshape((3, 1))  # Ensure rvec is 3x1\n",
    "                tvec = tvec.reshape((3, 1))  # Ensure tvec is 3x1\n",
    "\n",
    "                # Draw axis for each marker\n",
    "                cv2.drawFrameAxes(frame, camera_matrix, camera_distortion, rvec, tvec, 0.1)\n",
    "\n",
    "                # Convert translation from meters to millimeters\n",
    "                tvec_mm = tvec * 1000  # Convert to millimeters\n",
    "\n",
    "                # Calculate camera position\n",
    "                rmat = cv2.Rodrigues(rvec)[0]\n",
    "                camera_position = -np.matrix(rmat).T @ np.matrix(tvec)\n",
    "\n",
    "                # Extract scalar values from the translation vector in mm\n",
    "                tvec_str = \"x={:.2f} mm y={:.2f} mm z={:.2f} mm\".format(tvec_mm[0][0], tvec_mm[1][0], tvec_mm[2][0])\n",
    "                cv2.putText(frame, tvec_str, (20, 460), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Extract rotation angles\n",
    "                r = Rotation.from_rotvec([rvec[0][0], rvec[1][0], rvec[2][0]])\n",
    "                rot = r.as_euler('xyz', degrees=True)\n",
    "\n",
    "                tx = round(camera_position[0, 0] * 1000, 2)  # Convert to millimeters\n",
    "                ty = round(camera_position[1, 0] * 1000, 2)  # Convert to millimeters\n",
    "                tz = round(camera_position[2, 0] * 1000, 2)  # Convert to millimeters\n",
    "\n",
    "                rx = round(180 - rot[0], 2)\n",
    "                ry = round(rot[1], 2)\n",
    "                rz = round(rot[2], 2)\n",
    "\n",
    "                rot_str = \"rx={:.2f} ry={:.2f} rz={:.2f}\".format(rx, ry, rz)\n",
    "                cv2.putText(frame, rot_str, (20, 420), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Calculate and display FPS every 10 frames\n",
    "    frame_count += 1\n",
    "    if frame_count >= 10:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - prev_time\n",
    "        fps = frame_count / elapsed_time\n",
    "        frame_count = 0\n",
    "        prev_time = current_time\n",
    "\n",
    "    fps_str = \"FPS: {:.2f}\".format(fps)\n",
    "    cv2.putText(frame, fps_str, (20, 380), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "753e0463-7ecb-47b4-8814-1e3568a4d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def focalMM_to_focalPixel(focalMM, pixelPitch):\n",
    "    return focalMM / pixelPitch\n",
    "\n",
    "# Camera matrix and distortion coefficients\n",
    "camera_matrix = np.array([\n",
    "    [583.52371648, 0., 296.37015552], \n",
    "    [0., 627.94558759, 358.30354553],\n",
    "    [0., 0., 1.]\n",
    "])\n",
    "camera_distortion = np.array([0.37230818, -0.4038794, 0.13492957, -0.00807964, 0.30939183])\n",
    "\n",
    "# Define the dictionary and detector parameters\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "parameters = aruco.DetectorParameters()\n",
    "\n",
    "# Define the 3D coordinates of the marker corners in meters\n",
    "marker_size = 0.1  # size of the marker in meters\n",
    "obj_points = np.array([\n",
    "    [-marker_size / 2, marker_size / 2, 0],\n",
    "    [ marker_size / 2, marker_size / 2, 0],\n",
    "    [ marker_size / 2, -marker_size / 2, 0],\n",
    "    [-marker_size / 2, -marker_size / 2, 0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Read the image\n",
    "image_path = '5e9449ce-8fc2-4268-a3e7-804d97cad046.jpg'  # Replace with your image path\n",
    "frame = cv2.imread(image_path)\n",
    "\n",
    "if frame is not None:\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ArUco markers\n",
    "    detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "    corners, ids, rejected = detector.detectMarkers(gray_frame)\n",
    "\n",
    "    if ids is not None:\n",
    "        # Draw detected markers\n",
    "        aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            # Estimate pose of each marker using SOLVEPNP_IPPE_SQUARE\n",
    "            success, rvec, tvec = cv2.solvePnP(obj_points, corners[i][0], camera_matrix, camera_distortion, flags=cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "\n",
    "            if success:\n",
    "                rvec = rvec.reshape((3, 1))  # Ensure rvec is 3x1\n",
    "                tvec = tvec.reshape((3, 1))  # Ensure tvec is 3x1\n",
    "\n",
    "                # Draw axis for each marker\n",
    "                cv2.drawFrameAxes(frame, camera_matrix, camera_distortion, rvec, tvec, 0.1)\n",
    "\n",
    "                # Convert translation from meters to millimeters\n",
    "                tvec_mm = tvec * 1000  # Convert to millimeters\n",
    "\n",
    "                # Calculate camera position\n",
    "                rmat = cv2.Rodrigues(rvec)[0]\n",
    "                camera_position = -np.matrix(rmat).T @ np.matrix(tvec)\n",
    "\n",
    "                # Extract scalar values from the translation vector in mm\n",
    "                tvec_str = \"x={:.2f} mm y={:.2f} mm z={:.2f} mm\".format(tvec_mm[0][0], tvec_mm[1][0], tvec_mm[2][0])\n",
    "                \n",
    "                # Extract rotation angles\n",
    "                r = Rotation.from_rotvec([rvec[0][0], rvec[1][0], rvec[2][0]])\n",
    "                rot = r.as_euler('xyz', degrees=True)\n",
    "\n",
    "                tx = round(camera_position[0, 0] * 1000, 2)  # Convert to millimeters\n",
    "                ty = round(camera_position[1, 0] * 1000, 2)  # Convert to millimeters\n",
    "                tz = round(camera_position[2, 0] * 1000, 2)  # Convert to millimeters\n",
    "\n",
    "                rx = round(180 - rot[0], 2)\n",
    "                ry = round(rot[1], 2)\n",
    "                rz = round(rot[2], 2)\n",
    "\n",
    "                rot_str = \"rx={:.2f} ry={:.2f} rz={:.2f}\".format(rx, ry, rz)\n",
    "\n",
    "                # Display the text at the top of the image\n",
    "                cv2.putText(frame, tvec_str, (20, 30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, rot_str, (20, 70), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(f\"Failed to load image at {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afb7f5-b9d2-4c9a-9fb1-8edf0d57341b",
   "metadata": {},
   "source": [
    "# __3.Execute Image with ARUCO /\\ Object detection tflite__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "498ae972-e8a9-4123-aefe-9fff4d490ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import cv2.aruco as aruco\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def load_labels(label_path):\n",
    "    with open(label_path, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def set_input_tensor(interpreter, image):\n",
    "    tensor_index = interpreter.get_input_details()[0]['index']\n",
    "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "    input_tensor[:, :] = image\n",
    "\n",
    "def get_output_tensor(interpreter, index):\n",
    "    output_details = interpreter.get_output_details()[index]\n",
    "    tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "    return tensor\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "    set_input_tensor(interpreter, image)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    scores = get_output_tensor(interpreter, 0)\n",
    "    boxes = get_output_tensor(interpreter, 1)\n",
    "\n",
    "    results = []\n",
    "    for i in range(scores.shape[0]):\n",
    "        if scores[i] >= threshold:\n",
    "            result = {\n",
    "                'bounding_box': boxes[i],\n",
    "                'score': scores[i]\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def draw_bounding_boxes(image, results):\n",
    "    height, width, _ = image.shape\n",
    "    for result in results:\n",
    "        ymin, xmin, ymax, xmax = result['bounding_box']\n",
    "        ymin = int(max(1, ymin * height))\n",
    "        xmin = int(max(1, xmin * width))\n",
    "        ymax = int(min(height, ymax * height))\n",
    "        xmax = int(min(width, xmax * width))\n",
    "\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n",
    "        label = f\"{result['score']:.2f}\"\n",
    "        cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "def main():\n",
    "    model_path = \"detect.tflite\"\n",
    "    label_path = \"labels.txt\"\n",
    "    threshold = 0.5\n",
    "    image_path = \"0a0097e5-WhatsApp_Image_2024-07-28_at_4.36.09_PM_2.jpeg\"  # Update this path to your image\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    labels = load_labels(label_path)\n",
    "\n",
    "    # Load image\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        print(f\"Failed to load image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    # ArUco marker detection setup\n",
    "    camera_matrix = np.array([\n",
    "        [583.52371648, 0., 296.37015552], \n",
    "        [0., 627.94558759, 358.30354553],\n",
    "        [0., 0., 1.]\n",
    "    ])\n",
    "    camera_distortion = np.array([0.37230818, -0.4038794, 0.13492957, -0.00807964, 0.30939183])\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "    parameters = aruco.DetectorParameters()\n",
    "    marker_size = 0.1  # size of the marker in meters\n",
    "    obj_points = np.array([\n",
    "        [-marker_size / 2, marker_size / 2, 0],\n",
    "        [marker_size / 2, marker_size / 2, 0],\n",
    "        [marker_size / 2, -marker_size / 2, 0],\n",
    "        [-marker_size / 2, -marker_size / 2, 0]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # ArUco marker detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "    corners, ids, rejected = detector.detectMarkers(gray_frame)\n",
    "\n",
    "    if ids is not None:\n",
    "        aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "        for i in range(len(ids)):\n",
    "            success, rvec, tvec = cv2.solvePnP(obj_points, corners[i][0], camera_matrix, camera_distortion, flags=cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "            if success:\n",
    "                cv2.drawFrameAxes(frame, camera_matrix, camera_distortion, rvec, tvec, 0.1)\n",
    "                tvec_mm = tvec * 1000\n",
    "                rmat = cv2.Rodrigues(rvec)[0]\n",
    "                camera_position = -np.matrix(rmat).T @ np.matrix(tvec)\n",
    "                tvec_str = \"x={:.2f} mm y={:.2f} mm z={:.2f} mm\".format(tvec_mm[0][0], tvec_mm[1][0], tvec_mm[2][0])\n",
    "                r = Rotation.from_rotvec([rvec[0][0], rvec[1][0], rvec[2][0]])\n",
    "                rot = r.as_euler('xyz', degrees=True)\n",
    "                rx = round(180 - rot[0], 2)\n",
    "                ry = round(rot[1], 2)\n",
    "                rz = round(rot[2], 2)\n",
    "                rot_str = \"rx={:.2f} ry={:.2f} rz={:.2f}\".format(rx, ry, rz)\n",
    "                cv2.putText(frame, tvec_str, (20, 30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, rot_str, (20, 70), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Preprocessing for TFLite model input\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (640, 640))  # Update size based on model input\n",
    "    input_image = np.expand_dims(image_resized / 255.0, axis=0).astype(np.float32)  # Normalization\n",
    "\n",
    "    # Perform object detection\n",
    "    results = detect_objects(interpreter, input_image, threshold)\n",
    "    draw_bounding_boxes(frame, results)\n",
    "\n",
    "    # Display the frame with bounding boxes and ArUco data\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fd4b9-8b94-4b01-86df-64d0309e02f3",
   "metadata": {},
   "source": [
    "# __4.Execute Image with ARUCO /\\ Object detection tflite /\\ Distance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f829ccbd-22f2-4225-a4dc-e71da9c5af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import cv2.aruco as aruco\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def load_labels(label_path):\n",
    "    with open(label_path, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def set_input_tensor(interpreter, image):\n",
    "    tensor_index = interpreter.get_input_details()[0]['index']\n",
    "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "    input_tensor[:, :] = image\n",
    "\n",
    "def get_output_tensor(interpreter, index):\n",
    "    output_details = interpreter.get_output_details()[index]\n",
    "    tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "    return tensor\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "    set_input_tensor(interpreter, image)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    scores = get_output_tensor(interpreter, 0)\n",
    "    boxes = get_output_tensor(interpreter, 1)\n",
    "\n",
    "    results = []\n",
    "    for i in range(scores.shape[0]):\n",
    "        if scores[i] >= threshold:\n",
    "            result = {\n",
    "                'bounding_box': boxes[i],\n",
    "                'score': scores[i]\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def draw_bounding_boxes(image, results):\n",
    "    height, width, _ = image.shape\n",
    "    centers = []\n",
    "    for result in results:\n",
    "        ymin, xmin, ymax, xmax = result['bounding_box']\n",
    "        ymin = int(max(1, ymin * height))\n",
    "        xmin = int(max(1, xmin * width))\n",
    "        ymax = int(min(height, ymax * height))\n",
    "        xmax = int(min(width, xmax * width))\n",
    "\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n",
    "        label = f\"{result['score']:.2f}\"\n",
    "        cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # Calculate the center of the bounding box\n",
    "        center_x = (xmin + xmax) // 2\n",
    "        center_y = (ymin + ymax) // 2\n",
    "        centers.append((center_x, center_y))\n",
    "\n",
    "    return centers\n",
    "\n",
    "def main():\n",
    "    model_path = \"detect.tflite\"\n",
    "    label_path = \"labels.txt\"\n",
    "    threshold = 0.5\n",
    "    image_path = \"0a0097e5-WhatsApp_Image_2024-07-28_at_4.36.09_PM_2.jpeg\"  # Update this path to your image\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    labels = load_labels(label_path)\n",
    "\n",
    "    # Load image\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        print(f\"Failed to load image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    # ArUco marker detection setup\n",
    "    camera_matrix = np.array([\n",
    "        [583.52371648, 0., 296.37015552], \n",
    "        [0., 627.94558759, 358.30354553],\n",
    "        [0., 0., 1.]\n",
    "    ])\n",
    "    camera_distortion = np.array([0.37230818, -0.4038794, 0.13492957, -0.00807964, 0.30939183])\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "    parameters = aruco.DetectorParameters()\n",
    "    marker_size = 0.1  # size of the marker in meters\n",
    "    obj_points = np.array([\n",
    "        [-marker_size / 2, marker_size / 2, 0],\n",
    "        [marker_size / 2, marker_size / 2, 0],\n",
    "        [marker_size / 2, -marker_size / 2, 0],\n",
    "        [-marker_size / 2, -marker_size / 2, 0]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # ArUco marker detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "    corners, ids, rejected = detector.detectMarkers(gray_frame)\n",
    "\n",
    "    aruco_center = None\n",
    "    if ids is not None:\n",
    "        aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "        for i in range(len(ids)):\n",
    "            success, rvec, tvec = cv2.solvePnP(obj_points, corners[i][0], camera_matrix, camera_distortion, flags=cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "            if success:\n",
    "                cv2.drawFrameAxes(frame, camera_matrix, camera_distortion, rvec, tvec, 0.1)\n",
    "                tvec_mm = tvec * 1000\n",
    "                rmat = cv2.Rodrigues(rvec)[0]\n",
    "                camera_position = -np.matrix(rmat).T @ np.matrix(tvec)\n",
    "                tvec_str = \"x={:.2f} mm y={:.2f} mm z={:.2f} mm\".format(tvec_mm[0][0], tvec_mm[1][0], tvec_mm[2][0])\n",
    "                r = Rotation.from_rotvec([rvec[0][0], rvec[1][0], rvec[2][0]])\n",
    "                rot = r.as_euler('xyz', degrees=True)\n",
    "                rx = round(180 - rot[0], 2)\n",
    "                ry = round(rot[1], 2)\n",
    "                rz = round(rot[2], 2)\n",
    "                rot_str = \"rx={:.2f} ry={:.2f} rz={:.2f}\".format(rx, ry, rz)\n",
    "                cv2.putText(frame, tvec_str, (20, 30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, rot_str, (20, 70), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Calculate the center of the ArUco marker\n",
    "                aruco_center = np.mean(corners[i][0], axis=0).astype(int)\n",
    "\n",
    "    # Preprocessing for TFLite model input\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (640, 640))  # Update size based on model input\n",
    "    input_image = np.expand_dims(image_resized / 255.0, axis=0).astype(np.float32)  # Normalization\n",
    "\n",
    "    # Perform object detection\n",
    "    results = detect_objects(interpreter, input_image, threshold)\n",
    "    box_centers = draw_bounding_boxes(frame, results)\n",
    "\n",
    "    # Draw lines from the ArUco marker center to each bounding box center\n",
    "    if aruco_center is not None:\n",
    "        for center in box_centers:\n",
    "            cv2.line(frame, tuple(aruco_center), center, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with bounding boxes and ArUco data\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3807c386-7d30-4b15-88cb-1a9b3e7b57c8",
   "metadata": {},
   "source": [
    "## __5.Execute With ARUCO /\\ Object detection tflite /\\ Distance /\\ Export__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be60b6c4-6452-4d58-9967-f8f563c7ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import cv2.aruco as aruco\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def load_labels(label_path):\n",
    "    with open(label_path, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def set_input_tensor(interpreter, image):\n",
    "    tensor_index = interpreter.get_input_details()[0]['index']\n",
    "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "    input_tensor[:, :] = image\n",
    "\n",
    "def get_output_tensor(interpreter, index):\n",
    "    output_details = interpreter.get_output_details()[index]\n",
    "    tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "    return tensor\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "    set_input_tensor(interpreter, image)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    scores = get_output_tensor(interpreter, 0)\n",
    "    boxes = get_output_tensor(interpreter, 1)\n",
    "\n",
    "    results = []\n",
    "    for i in range(scores.shape[0]):\n",
    "        if scores[i] >= threshold:\n",
    "            result = {\n",
    "                'bounding_box': boxes[i],\n",
    "                'score': scores[i]\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def draw_bounding_boxes(image, results):\n",
    "    height, width, _ = image.shape\n",
    "    centers = []\n",
    "    for result in results:\n",
    "        ymin, xmin, ymax, xmax = result['bounding_box']\n",
    "        ymin = int(max(1, ymin * height))\n",
    "        xmin = int(max(1, xmin * width))\n",
    "        ymax = int(min(height, ymax * height))\n",
    "        xmax = int(min(width, xmax * width))\n",
    "\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 255), 2)  # Yellow color for bounding boxes\n",
    "        label = f\"{result['score']:.2f}\"\n",
    "        cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)  # Yellow color for confidence values\n",
    "\n",
    "        # Calculate the center of the bounding box\n",
    "        center_x = (xmin + xmax) // 2\n",
    "        center_y = (ymin + ymax) // 2\n",
    "        centers.append((center_x, center_y))\n",
    "\n",
    "    return centers\n",
    "\n",
    "def crop_and_save_leaves(image, results, output_dir='cropped_leaves'):\n",
    "    height, width, _ = image.shape\n",
    "    leaf_images = []\n",
    "    for idx, result in enumerate(results):\n",
    "        ymin, xmin, ymax, xmax = result['bounding_box']\n",
    "        ymin = int(max(1, ymin * height))\n",
    "        xmin = int(max(1, xmin * width))\n",
    "        ymax = int(min(height, ymax * height))\n",
    "        xmax = int(min(width, xmax * width))\n",
    "\n",
    "        # Crop the bounding box region from the image\n",
    "        leaf_image = image[ymin:ymax, xmin:xmax]\n",
    "        leaf_images.append(leaf_image)\n",
    "\n",
    "        # Save the cropped leaf image\n",
    "        output_path = f\"{output_dir}/leaf_{idx}.jpg\"\n",
    "        cv2.imwrite(output_path, leaf_image)\n",
    "    \n",
    "    return leaf_images\n",
    "\n",
    "def main():\n",
    "    model_path = \"detect.tflite\"\n",
    "    label_path = \"labels.txt\"\n",
    "    threshold = 0.5\n",
    "    image_path = \"0a0097e5-WhatsApp_Image_2024-07-28_at_4.36.09_PM_2.jpeg\"  # Update this path to your image\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    labels = load_labels(label_path)\n",
    "\n",
    "    # Load image\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        print(f\"Failed to load image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    # ArUco marker detection setup\n",
    "    camera_matrix = np.array([\n",
    "        [583.52371648, 0., 296.37015552], \n",
    "        [0., 627.94558759, 358.30354553],\n",
    "        [0., 0., 1.]\n",
    "    ])\n",
    "    camera_distortion = np.array([0.37230818, -0.4038794, 0.13492957, -0.00807964, 0.30939183])\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "    parameters = aruco.DetectorParameters()\n",
    "    marker_size = 0.1  # size of the marker in meters\n",
    "    obj_points = np.array([\n",
    "        [-marker_size / 2, marker_size / 2, 0],\n",
    "        [marker_size / 2, marker_size / 2, 0],\n",
    "        [marker_size / 2, -marker_size / 2, 0],\n",
    "        [-marker_size / 2, -marker_size / 2, 0]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # ArUco marker detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "    corners, ids, rejected = detector.detectMarkers(gray_frame)\n",
    "\n",
    "    aruco_center = None\n",
    "    if ids is not None:\n",
    "        aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "        for i in range(len(ids)):\n",
    "            success, rvec, tvec = cv2.solvePnP(obj_points, corners[i][0], camera_matrix, camera_distortion, flags=cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "            if success:\n",
    "                cv2.drawFrameAxes(frame, camera_matrix, camera_distortion, rvec, tvec, 0.1)\n",
    "                tvec_mm = tvec * 1000\n",
    "                rmat = cv2.Rodrigues(rvec)[0]\n",
    "                camera_position = -np.matrix(rmat).T @ np.matrix(tvec)\n",
    "                tvec_str = \"x={:.2f} mm y={:.2f} mm z={:.2f} mm\".format(tvec_mm[0][0], tvec_mm[1][0], tvec_mm[2][0])\n",
    "                r = Rotation.from_rotvec([rvec[0][0], rvec[1][0], rvec[2][0]])\n",
    "                rot = r.as_euler('xyz', degrees=True)\n",
    "                rx = round(180 - rot[0], 2)\n",
    "                ry = round(rot[1], 2)\n",
    "                rz = round(rot[2], 2)\n",
    "                rot_str = \"rx={:.2f} ry={:.2f} rz={:.2f}\".format(rx, ry, rz)\n",
    "                cv2.putText(frame, tvec_str, (20, 30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, rot_str, (20, 70), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Calculate the center of the ArUco marker\n",
    "                aruco_center = np.mean(corners[i][0], axis=0).astype(int)\n",
    "\n",
    "    # Preprocessing for TFLite model input\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (640, 640))  # Update size based on model input\n",
    "    input_image = np.expand_dims(image_resized / 255.0, axis=0).astype(np.float32)  # Normalization\n",
    "\n",
    "    # Perform object detection\n",
    "    results = detect_objects(interpreter, input_image, threshold)\n",
    "    box_centers = draw_bounding_boxes(frame, results)\n",
    "\n",
    "    # Crop and save leaf images\n",
    "    crop_and_save_leaves(frame, results)\n",
    "\n",
    "    # Draw lines from the ArUco marker center to each bounding box center\n",
    "    if aruco_center is not None:\n",
    "        for center in box_centers:\n",
    "            cv2.line(frame, tuple(aruco_center), center, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with bounding boxes and ArUco data\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9662714-0f13-4fdf-aac6-d83f4b43fd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5eaaf53-699a-4857-a4a7-3c8cbc2aabe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores shape: (10,)\n",
      "Boxes shape: (10, 4)\n",
      "Classes shape: ()\n",
      "Detected Objects: [{'bounding_box': array([0.29976922, 0.42708132, 0.39245367, 0.49433598], dtype=float32), 'score': 0.99456084, 'label_id': 10}, {'bounding_box': array([0.21030797, 0.40645975, 0.26754925, 0.47419035], dtype=float32), 'score': 0.9932978, 'label_id': 10}, {'bounding_box': array([0.3966902 , 0.15968618, 0.46919703, 0.2135821 ], dtype=float32), 'score': 0.98756444, 'label_id': 10}, {'bounding_box': array([0.26730022, 0.47824264, 0.35140058, 0.55542517], dtype=float32), 'score': 0.98134744, 'label_id': 10}, {'bounding_box': array([0.4784892 , 0.19943172, 0.625328  , 0.28374678], dtype=float32), 'score': 0.97706556, 'label_id': 10}, {'bounding_box': array([0.4112168 , 0.35256162, 0.4603334 , 0.40973184], dtype=float32), 'score': 0.9680633, 'label_id': 10}, {'bounding_box': array([0.32386237, 0.21596752, 0.36344522, 0.29107404], dtype=float32), 'score': 0.9433292, 'label_id': 10}, {'bounding_box': array([0.41253722, 0.22016737, 0.47743613, 0.2992861 ], dtype=float32), 'score': 0.92484003, 'label_id': 10}, {'bounding_box': array([0.36080462, 0.27467668, 0.46120352, 0.34343606], dtype=float32), 'score': 0.9233334, 'label_id': 10}, {'bounding_box': array([0.2263852 , 0.47673595, 0.26894772, 0.53001845], dtype=float32), 'score': 0.6620721, 'label_id': 10}]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 173\u001b[0m\n\u001b[0;32m    170\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 173\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 160\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Perform object detection\u001b[39;00m\n\u001b[0;32m    159\u001b[0m results \u001b[38;5;241m=\u001b[39m detect_objects(interpreter, input_image, threshold)\n\u001b[1;32m--> 160\u001b[0m box_centers \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m crop_and_save_leaves(frame, results)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aruco_center \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[13], line 65\u001b[0m, in \u001b[0;36mdraw_bounding_boxes\u001b[1;34m(image, results, labels)\u001b[0m\n\u001b[0;32m     62\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(image, (xmin, ymin), (xmax, ymax), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Draw the label and confidence score\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_id\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(image, label, (xmin, ymin \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Calculate the center of the bounding box\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import cv2.aruco as aruco\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def load_labels(label_path):\n",
    "    with open(label_path, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def set_input_tensor(interpreter, image):\n",
    "    tensor_index = interpreter.get_input_details()[0]['index']\n",
    "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "    input_tensor[:, :] = image\n",
    "\n",
    "def get_output_tensor(interpreter, index):\n",
    "    output_details = interpreter.get_output_details()[index]\n",
    "    tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
    "    return tensor\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "    set_input_tensor(interpreter, image)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    scores = get_output_tensor(interpreter, 0)\n",
    "    boxes = get_output_tensor(interpreter, 1)\n",
    "    classes = get_output_tensor(interpreter, 2)  # Assuming this is where the classes are stored\n",
    "\n",
    "    print(f\"Scores shape: {scores.shape}\")\n",
    "    print(f\"Boxes shape: {boxes.shape}\")\n",
    "    print(f\"Classes shape: {classes.shape}\")\n",
    "\n",
    "    results = []\n",
    "    for i in range(scores.shape[0]):\n",
    "        if scores[i] >= threshold:\n",
    "            if len(classes.shape) > 0:  # Check if 'classes' is an array\n",
    "                label_id = int(classes[i])\n",
    "            else:\n",
    "                label_id = int(classes)  # If scalar, use it directly\n",
    "\n",
    "            result = {\n",
    "                'bounding_box': boxes[i],\n",
    "                'score': scores[i],\n",
    "                'label_id': label_id\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "    print(f\"Detected Objects: {results}\")\n",
    "    return results\n",
    "\n",
    "def draw_bounding_boxes(image, results, labels):\n",
    "    height, width, _ = image.shape\n",
    "    centers = []\n",
    "    for result in results:\n",
    "        ymin, xmin, ymax, xmax = result['bounding_box']\n",
    "        ymin = int(max(1, ymin * height))\n",
    "        xmin = int(max(1, xmin * width))\n",
    "        ymax = int(min(height, ymax * height))\n",
    "        xmax = int(min(width, xmax * width))\n",
    "\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 255), 2)\n",
    "\n",
    "        # Draw the label and confidence score\n",
    "        label = f\"{labels[result['label_id']]}: {result['score']:.2f}\"\n",
    "        cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "\n",
    "        # Calculate the center of the bounding box\n",
    "        center_x = (xmin + xmax) // 2\n",
    "        center_y = (ymin + ymax) // 2\n",
    "        centers.append((center_x, center_y))\n",
    "\n",
    "    return centers\n",
    "\n",
    "def crop_and_save_leaves(image, results, output_dir='cropped_leaves'):\n",
    "    height, width, _ = image.shape\n",
    "    leaf_images = []\n",
    "    for idx, result in enumerate(results):\n",
    "        ymin, xmin, ymax, xmax = result['bounding_box']\n",
    "        ymin = int(max(1, ymin * height))\n",
    "        xmin = int(max(1, xmin * width))\n",
    "        ymax = int(min(height, ymax * height))\n",
    "        xmax = int(min(width, xmax * width))\n",
    "\n",
    "        leaf_image = image[ymin:ymax, xmin:xmax]\n",
    "        leaf_images.append(leaf_image)\n",
    "\n",
    "        output_path = f\"{output_dir}/leaf_{idx}.jpg\"\n",
    "        cv2.imwrite(output_path, leaf_image)\n",
    "    \n",
    "    return leaf_images\n",
    "\n",
    "def main():\n",
    "    model_path = \"detect.tflite\"\n",
    "    label_path = \"labels.txt\"\n",
    "    threshold = 0.5\n",
    "    image_path = \"0a0097e5-WhatsApp_Image_2024-07-28_at_4.36.09_PM_2.jpeg\"\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    labels = load_labels(label_path)\n",
    "\n",
    "    # Load image\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        print(f\"Failed to load image at {image_path}\")\n",
    "        return\n",
    "\n",
    "    # ArUco marker detection setup\n",
    "    camera_matrix = np.array([\n",
    "        [583.52371648, 0., 296.37015552], \n",
    "        [0., 627.94558759, 358.30354553],\n",
    "        [0., 0., 1.]\n",
    "    ])\n",
    "    camera_distortion = np.array([0.37230818, -0.4038794, 0.13492957, -0.00807964, 0.30939183])\n",
    "    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_250)\n",
    "    parameters = aruco.DetectorParameters()\n",
    "    marker_size = 0.1  # size of the marker in meters\n",
    "    obj_points = np.array([\n",
    "        [-marker_size / 2, marker_size / 2, 0],\n",
    "        [marker_size / 2, marker_size / 2, 0],\n",
    "        [marker_size / 2, -marker_size / 2, 0],\n",
    "        [-marker_size / 2, -marker_size / 2, 0]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    detector = aruco.ArucoDetector(aruco_dict, parameters)\n",
    "    corners, ids, rejected = detector.detectMarkers(gray_frame)\n",
    "\n",
    "    aruco_center = None\n",
    "    if ids is not None:\n",
    "        aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "        for i in range(len(ids)):\n",
    "            success, rvec, tvec = cv2.solvePnP(obj_points, corners[i][0], camera_matrix, camera_distortion, flags=cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "            if success:\n",
    "                cv2.drawFrameAxes(frame, camera_matrix, camera_distortion, rvec, tvec, 0.1)\n",
    "                tvec_mm = tvec * 1000\n",
    "                rmat = cv2.Rodrigues(rvec)[0]\n",
    "                camera_position = -np.matrix(rmat).T @ np.matrix(tvec)\n",
    "                tvec_str = \"x={:.2f} mm y={:.2f} mm z={:.2f} mm\".format(tvec_mm[0][0], tvec_mm[1][0], tvec_mm[2][0])\n",
    "                r = Rotation.from_rotvec([rvec[0][0], rvec[1][0], rvec[2][0]])\n",
    "                rot = r.as_euler('xyz', degrees=True)\n",
    "                rx = round(180 - rot[0], 2)\n",
    "                ry = round(rot[1], 2)\n",
    "                rz = round(rot[2], 2)\n",
    "                rot_str = \"rx={:.2f} ry={:.2f} rz={:.2f}\".format(rx, ry, rz)\n",
    "                cv2.putText(frame, tvec_str, (20, 30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(frame, rot_str, (20, 70), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                aruco_center = np.mean(corners[i][0], axis=0).astype(int)\n",
    "\n",
    "    # Preprocessing for TFLite model input\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (640, 640))\n",
    "    input_image = np.expand_dims(image_resized / 255.0, axis=0).astype(np.float32)\n",
    "\n",
    "    # Perform object detection\n",
    "    results = detect_objects(interpreter, input_image, threshold)\n",
    "    box_centers = draw_bounding_boxes(frame, results, labels)\n",
    "\n",
    "    crop_and_save_leaves(frame, results)\n",
    "\n",
    "    if aruco_center is not None:\n",
    "        for center in box_centers:\n",
    "            cv2.line(frame, tuple(aruco_center), center, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fb022-c15f-4727-b1f9-a73d7c382f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
